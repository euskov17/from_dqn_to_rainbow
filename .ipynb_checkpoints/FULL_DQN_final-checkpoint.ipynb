{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MIoT_lu5QI0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.wrappers\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "\n",
    "import utils\n",
    "from framebuffer import FrameBuffer\n",
    "from replay_buffer import ReplayBuffer\n",
    "import atari_wrappers\n",
    "from experience_replay_buffer import ExperienceReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"BreakoutNoFrameskip-v4\"\n",
    "PARAMS =  {'is_dueling':False, 'is_distributional':False, 'is_noisy': False, 'is_double': False}\n",
    "\n",
    "USE_PRIORITIEZED_BUFFER = True\n",
    "\n",
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 3 * 10 ** 4\n",
    "N_STEPS = 1000\n",
    "\n",
    "timesteps_per_epoch = 1\n",
    "\n",
    "batch_size = 16\n",
    "total_steps = 10 ** 6\n",
    "decay_steps = 7 * 10 ** 5\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 50\n",
    "refresh_target_network_freq = 5000\n",
    "eval_freq = 5000\n",
    "\n",
    "max_grad_norm = 50\n",
    "\n",
    "n_lives = 5\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAN-8G-i5QI3"
   },
   "outputs": [],
   "source": [
    "class PreprocessAtariObs(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        ObservationWrapper.__init__(self, env)\n",
    "        self.img_size = (1, 64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, self.img_size)\n",
    "\n",
    "    def _to_gray_scale(self, rgb, channel_weights=[0.8, 0.1, 0.1]):\n",
    "        return rgb @ np.array(channel_weights)\n",
    "\n",
    "    def observation(self, img):\n",
    "        return np.expand_dims(cv2.resize(self._to_gray_scale(img)[25:201, :], self.img_size[1:]).astype(np.float32) / 255, axis=0)\n",
    "\n",
    "\n",
    "def PrimaryAtariWrap(env, clip_rewards=True):\n",
    "    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n",
    "    env = atari_wrappers.EpisodicLifeEnv(env)\n",
    "    env = atari_wrappers.FireResetEnv(env)\n",
    "    if clip_rewards:\n",
    "        env = atari_wrappers.ClipRewardEnv(env)\n",
    "    env = PreprocessAtariObs(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_env(clip_rewards=True, seed=None):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    env = PrimaryAtariWrap(env, clip_rewards)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHnHEnSQ5QI7",
    "outputId": "498c8ee4-881a-4fd9-fc26-b2c5bee4db83"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noisy_layer(nn.Module):\n",
    "    def __init__(self, input, output, sigma = 0.5):\n",
    "        super().__init__()\n",
    "        self.sigma  = sigma\n",
    "        self.input  = input\n",
    "        self.output = output\n",
    "\n",
    "        self.mu_bias        = nn.Parameter(torch.FloatTensor(output))\n",
    "        self.mu_weight      = nn.Parameter(torch.FloatTensor(output, input))\n",
    "        self.sigma_bias     = nn.Parameter(torch.FloatTensor(output))\n",
    "        self.sigma_weight   = nn.Parameter(torch.FloatTensor(output, input))\n",
    "\n",
    "        self.bound = input ** (-0.5)\n",
    "        self.mu_bias.data.uniform_(-self.bound, self.bound)\n",
    "        self.sigma_bias.data.fill_(self.sigma * self.bound)\n",
    "        self.mu_weight.data.uniform_(-self.bound, self.bound)\n",
    "        self.sigma_weight.data.fill_(self.sigma * self.bound)\n",
    "\n",
    "        self.epsilon_input  = self.get_noise(self.input)\n",
    "        self.epsilon_output = self.get_noise(self.output)\n",
    "    \n",
    "    def get_noise(self, features):\n",
    "        noise = torch.FloatTensor(features).uniform_(-self.bound, self.bound).to(self.mu_bias.device)\n",
    "        return torch.sign(noise) * torch.sqrt(torch.abs(noise))\n",
    "    \n",
    "    def forward(self, x, sample_noise = True):\n",
    "        if not self.training:\n",
    "            return nn.functional.linear(x, weight = self.mu_weight, bias = self.mu_bias)\n",
    "\n",
    "        if sample_noise:\n",
    "            self.epsilon_input = self.get_noise(self.input)\n",
    "            self.epsilon_output = self.get_noise(self.output)\n",
    "\n",
    "        weight = self.sigma_weight * torch.ger(self.epsilon_output, self.epsilon_input) + self.mu_weight\n",
    "        bias = self.sigma_bias * self.epsilon_output + self.mu_bias\n",
    "        return nn.functional.linear(x, weight = weight, bias = bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4WtiBw75QI7"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0, \n",
    "                 is_dueling=False, is_distributional=False, is_noisy=False, is_double=False, distribution_len = 15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        self.output = n_actions * (1 + (distribution_len - 1) * is_distributional) + is_dueling\n",
    "        self.is_dueling         = is_dueling\n",
    "        self.is_distributional  = is_distributional\n",
    "        self.is_noisy           = is_noisy\n",
    "        self.is_double          = is_double\n",
    "        self.distribution_len = distribution_len\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.state_shape[0], out_channels=16, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        if is_noisy:\n",
    "            self.dense_layers = nn.Sequential(\n",
    "                Noisy_layer(64 * 7 * 7, 256),\n",
    "                nn.ReLU(),\n",
    "                Noisy_layer(256, self.output)\n",
    "            )\n",
    "        else:\n",
    "            self.dense_layers = nn.Sequential(\n",
    "                nn.Linear(64 * 7 * 7, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, self.output)\n",
    "            )\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        model_device = next(self.parameters()).device\n",
    "        if self.is_distributional:\n",
    "            state_t = torch.tensor(state_t, device=model_device, dtype=torch.float32)\n",
    "            batch_size = state_t.shape[0]         \n",
    "            result = self.dense_layers(self.conv_layers(state_t))\n",
    "\n",
    "            v_value, advantage = result[:, 0], result[:, 1:]\n",
    "\n",
    "            advantage = advantage.reshape(batch_size, self.n_actions, self.distribution_len)\n",
    "\n",
    "            advantage = advantage - torch.mean(advantage, dim=(1, 2))[:, None, None]\n",
    "            distribution = v_value[:, None, None] + advantage\n",
    "            distribution = distribution.reshape(batch_size, self.n_actions, self.distribution_len)\n",
    "\n",
    "            return distribution\n",
    "        \n",
    "        if self.is_dueling:\n",
    "            pre_result = self.dense_layers(self.conv_layers(state_t))\n",
    "            v_value = pre_result[:, 0]\n",
    "            advantage = pre_result[:, 1:]\n",
    "            advantage = advantage - torch.mean(advantage, axis=1)[:, None]\n",
    "            qvalues = advantage + v_value[:, None]\n",
    "        else:\n",
    "            qvalues = self.dense_layers(self.conv_layers(state_t))\n",
    "\n",
    "        return qvalues\n",
    "    \n",
    "    def get_z_values(self, states):\n",
    "        distribution = self.forward(states)\n",
    "        return distribution\n",
    "\n",
    "    def get_qvalues(self, states=None, zvalues=None):\n",
    "        if self.is_distributional:\n",
    "            if zvalues is None:\n",
    "                distributions = self.get_z_values(states)\n",
    "            else:\n",
    "                distributions = zvalues\n",
    "            qvalues = distributions.mean(-1)\n",
    "        else:\n",
    "            model_device = next(self.parameters()).device\n",
    "            states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "            qvalues = self.forward(states)\n",
    "        \n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    def sample_actions(self, qvalues, use_epsilon=True):\n",
    "        epsilon = self.epsilon if use_epsilon else 0\n",
    "        \n",
    "        if len(qvalues.shape) == 1:\n",
    "            batch_size = 1\n",
    "            n_actions = len(qvalues)\n",
    "        else:\n",
    "            batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yW52hELy5QI7"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU3udzMx5QI8"
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, target_network, env, exp_replay, n_steps=1, gamma=.99, is_priority=USE_PRIORITIEZED_BUFFER):\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    qvalues = agent.get_qvalues([s])\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if is_priority:\n",
    "            q_s_a = qvalues[0, a]\n",
    "        \n",
    "        qvalues = agent.get_qvalues([next_s])\n",
    "        \n",
    "        if is_priority:\n",
    "            next_a = agent.sample_actions(qvalues[0], use_epsilon=False)\n",
    "            delta = np.abs(q_s_a - (r + (1 - done) * gamma * target_network.get_qvalues([next_s])[0, next_a]))\n",
    "            exp_replay.add(s, a, r, next_s, done, delta)\n",
    "        else:\n",
    "            exp_replay.add(s, a, r, next_s, done)\n",
    "\n",
    "        s = next_s \n",
    "        \n",
    "        if done:\n",
    "            env.reset()\n",
    "    \n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-33_pYt5QI9"
   },
   "outputs": [],
   "source": [
    "kappa = 10\n",
    "\n",
    "def huber_estimator(u):\n",
    "    squerred = 1 / 2 * u ** 2\n",
    "    otherwise = kappa * (torch.abs(u) - kappa / 2)\n",
    "    return torch.where(torch.abs(u) <= kappa, squerred, otherwise)\n",
    "\n",
    "def rho(u, tau):\n",
    "    indicator = (u < 0).float()\n",
    "    return huber_estimator(u) * torch.abs(tau - indicator) \n",
    "\n",
    "def compute_loss(pred, target, weights=None):\n",
    "    N = pred.shape[1] \n",
    "    tau = (torch.arange(N, device=device) / N + 1 / (2 * N))\n",
    "    batch_size = pred.shape[0]\n",
    "    \n",
    "    diff_tensor = torch.zeros((batch_size, N, N), device=device)\n",
    "    \n",
    "    diff_tensor = target.unsqueeze(2) - pred[:, None]\n",
    "    if weights is None:\n",
    "        loss = rho(diff_tensor, tau[None, :, None]).sum() / N\n",
    "    else:\n",
    "        loss = (weights * rho(diff_tensor, tau[None, :, None]).sum((1, 2))).sum() / N  \n",
    "    return loss\n",
    "\n",
    "def compute_td_loss(states, actions, rewards, next_states, is_done, weights=None,\n",
    "                    agent=None, target_network=None,\n",
    "                    gamma=0.99,\n",
    "                    device=device):\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "    weights = torch.tensor(rewards, device=device, dtype=torch.float32).detach() if weights is not None else None\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    \n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    is_not_done = 1 - is_done\n",
    "    \n",
    "    \n",
    "    if agent.is_distributional:\n",
    "        predicted_z_values = agent.get_z_values(states)\n",
    "        predicted_q_values = agent.get_qvalues(next_states)\n",
    "        predicted_next_action = np.argmax(predicted_q_values, 1) \n",
    "        predicted_zvalues_for_actions = predicted_z_values[range(len(actions)), actions, :]\n",
    "\n",
    "        target_next_z_values = target_network.get_z_values(next_states)\n",
    "        target_next_z_values_for_action = target_next_z_values[range(len(actions)), predicted_next_action, :]\n",
    "        target = rewards[:, None] + gamma * target_next_z_values_for_action * is_not_done[:, None]\n",
    "\n",
    "        loss = compute_loss(predicted_zvalues_for_actions, target.detach(), weights)\n",
    "        return loss\n",
    "    \n",
    "    predicted_qvalues = agent(states)\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    if agent.is_double:\n",
    "        next_actions = torch.max(agent(next_states), 1).indices\n",
    "        next_state_values = predicted_next_qvalues[range(len(actions)), next_actions]\n",
    "    else:\n",
    "        next_state_values = torch.max(predicted_next_qvalues, 1).values\n",
    "\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    \n",
    "    if weights is not None:\n",
    "        loss  = torch.mean(weights * (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "    else:\n",
    "        loss  = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toRo-frr5QI-"
   },
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_shape, n_actions, epsilon = 1, **PARAMS).to(device)\n",
    "opt = torch.optim.Adam(agent.parameters(), lr = 1e-4)\n",
    "\n",
    "target_network = DQNAgent(state_shape, n_actions, **PARAMS).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.load('checkpoints/model_step_500000.pt')\n",
    "# mean_rw_history = data['mean_rw_history']\n",
    "# td_loss_history = data['td_loss_history']\n",
    "# grad_norm_history = data['grad_norm_history']\n",
    "# initial_state_v_history = data['initial_state_v_history']\n",
    "# step = data['step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QntZGifw5QI-"
   },
   "outputs": [],
   "source": [
    "exp_replay = ExperienceReplayBuffer(REPLAY_BUFFER_SIZE, 0.6, 0.4) if USE_PRIORITIEZED_BUFFER else ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in trange(REPLAY_BUFFER_SIZE // N_STEPS):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available! \n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, target_network, env, exp_replay, n_steps=N_STEPS)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aaVrMrm5QI_"
   },
   "outputs": [],
   "source": [
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1eTcwdtXjfC"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "beta_diff = (1 - exp_replay.beta) /  total_steps\n",
    "\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "        _, state = play_and_record(state, agent, target_network, env, exp_replay, timesteps_per_epoch)\n",
    "        batch = exp_replay.sample(batch_size)\n",
    "        loss = compute_td_loss(*batch, agent = agent, target_network=target_network)\n",
    "        loss.backward()\n",
    "        \n",
    "        if USE_PRIORITIEZED_BUFFER:\n",
    "            exp_replay.beta += beta_diff\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm.cpu())\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(clip_rewards=False, seed=step), agent, n_games=3 * n_lives, greedy=True)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per life\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()\n",
    "        if step and step % 10**5 == 0:\n",
    "            torch.save(\n",
    "            {\n",
    "                'agent_state_dict': agent.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'mean_rw_history': mean_rw_history,\n",
    "                'td_loss_history': td_loss_history,\n",
    "                'grad_norm_history': grad_norm_history,\n",
    "                'initial_state_v_history': initial_state_v_history,\n",
    "                'step': step                     \n",
    "            },\n",
    "                os.path.join('checkpoints', f'model_step_{step}.pt'),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z63E_AsE5QJA"
   },
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(clip_rewards=False, seed=9),\n",
    "    agent, n_games=30, greedy=True, t_max=10 * 1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_render(env):\n",
    "    env.metadata['render.modes'] = ['rgb_array']\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAku_zV35QJA"
   },
   "outputs": [],
   "source": [
    "gym.logger.set_level(gym.logger.DEBUG)\n",
    "with gym.wrappers.Monitor(patch_render(make_env()), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, agent, n_games=n_lives, greedy=True) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCf7Cy565QJA"
   },
   "outputs": [],
   "source": [
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q23wlez5QJB"
   },
   "outputs": [],
   "source": [
    "eval_env = make_env(clip_rewards=False)\n",
    "record = utils.play_and_log_episode(eval_env, agent)\n",
    "print('total reward for life:', np.sum(record['rewards']))\n",
    "for key in record:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSc4uViV5QJB"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(record['v_mc'], record['v_agent'])\n",
    "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
    "       'black', linestyle='--', label='x=y')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('State Value Estimates')\n",
    "ax.set_xlabel('Monte-Carlo')\n",
    "ax.set_ylabel('Agent')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2qojbM7a5QI-"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
